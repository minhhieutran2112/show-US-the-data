{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2021-06-03T20:58:21.479328Z","iopub.status.busy":"2021-06-03T20:58:21.47903Z","iopub.status.idle":"2021-06-03T20:58:41.287842Z","shell.execute_reply":"2021-06-03T20:58:41.286981Z","shell.execute_reply.started":"2021-06-03T20:58:21.479257Z"},"scrolled":true,"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["I0614 21:16:16.626769  9896 filelock.py:274] Lock 1832085578248 acquired on C:\\Users\\mt601/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b.lock\n","Downloading: 100%|██████████| 481/481 [00:00<00:00, 241kB/s]\n","I0614 21:16:16.973569  9896 filelock.py:318] Lock 1832085578248 released on C:\\Users\\mt601/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b.lock\n","I0614 21:16:17.313405  9896 filelock.py:274] Lock 1832085578920 acquired on C:\\Users\\mt601/.cache\\huggingface\\transformers\\d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab.lock\n","Downloading: 100%|██████████| 899k/899k [00:00<00:00, 3.61MB/s]\n","I0614 21:16:17.910042  9896 filelock.py:318] Lock 1832085578920 released on C:\\Users\\mt601/.cache\\huggingface\\transformers\\d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab.lock\n","I0614 21:16:18.240848  9896 filelock.py:274] Lock 1832085578976 acquired on C:\\Users\\mt601/.cache\\huggingface\\transformers\\cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\n","Downloading: 100%|██████████| 456k/456k [00:00<00:00, 2.95MB/s]\n","I0614 21:16:18.736563  9896 filelock.py:318] Lock 1832085578976 released on C:\\Users\\mt601/.cache\\huggingface\\transformers\\cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\n","I0614 21:16:19.071374  9896 filelock.py:274] Lock 1832085578920 acquired on C:\\Users\\mt601/.cache\\huggingface\\transformers\\d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730.lock\n","Downloading: 100%|██████████| 1.36M/1.36M [00:00<00:00, 4.40MB/s]\n","I0614 21:16:19.733993  9896 filelock.py:318] Lock 1832085578920 released on C:\\Users\\mt601/.cache\\huggingface\\transformers\\d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730.lock\n"]},{"data":{"text/plain":["<torch._C.Generator at 0x1aaff993330>"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["import warnings\r\n","warnings.filterwarnings('ignore', category=DeprecationWarning)\r\n","warnings.filterwarnings('ignore', category=FutureWarning)\r\n","\r\n","import os\r\n","import re\r\n","import json\r\n","import glob\r\n","from copy import deepcopy\r\n","from collections import defaultdict\r\n","from functools import partial\r\n","from imblearn.under_sampling import RandomUnderSampler\r\n","\r\n","import pandas as pd\r\n","import numpy as np\r\n","\r\n","from nltk import sent_tokenize\r\n","\r\n","import matplotlib.pyplot as plt\r\n","import seaborn as sns\r\n","\r\n","import unidecode\r\n","\r\n","from tqdm.notebook import tqdm\r\n","import string\r\n","\r\n","from transformers import AutoTokenizer, AutoModel\r\n","import torch\r\n","from torch import nn\r\n","from torchcrf import CRF\r\n","\r\n","from sklearn.model_selection import train_test_split\r\n","\r\n","%matplotlib inline\r\n","\r\n","device='cuda' if torch.cuda.is_available() else 'cpu'\r\n","model_checkpoint='bert-base-uncased'\r\n","tokenizer=AutoTokenizer.from_pretrained(model_checkpoint)\r\n","\r\n","def clean_text(txt):\r\n","    return [re.sub('[^A-Za-z0-9]+', ' ', t.lower()) for t in txt]\r\n","\r\n","torch.manual_seed(1)"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# train_df=pd.read_csv('../input/show-me-the-data-preparer/processed_train_df.csv')\r\n","# sample_sub = pd.read_csv('../input/show-me-the-data-preparer/processed_test_df.csv')\r\n","\r\n","train_df=pd.read_csv('data/processed/processed_train_df.csv').reset_index()\r\n","sample_sub = pd.read_csv('data/processed/processed_test_df.csv')\r\n","\r\n","train_df.drop_duplicates(inplace=True)\r\n","# train_df.dropna(inplace=True)\r\n","train_df.label=train_df.label.fillna('')\r\n","labels=[int(len(label)>0) for label in train_df.label]\r\n","\r\n","rus=RandomUnderSampler()\r\n","X_res, y_res = rus.fit_resample(train_df.iloc[:,:2], labels)\r\n","train_df=pd.merge(train_df,X_res.drop('text',axis=1),on='index').set_index('index')\r\n","\r\n","train_df=train_df[[len(str(t)) > 50 for t in train_df.text]]\r\n","train_df['cls_label']=[int(len(t)>0) for t in train_df.label]\r\n","\r\n","train_df.reset_index(drop=True,inplace=True)\r\n","\r\n","X_train, X_val, y_train, y_val = train_test_split(train_df.text, train_df.label, random_state=1821)\r\n","y_train_cls=train_df.cls_label[X_train.index]\r\n","y_val_cls=train_df.cls_label[X_val.index]"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["START_TAG=tokenizer.cls_token\r\n","STOP_TAG=tokenizer.sep_token\r\n","PAD_TAG=tokenizer.pad_token\r\n","\r\n","tag_to_ix = {\"B\": 0, \"I\": 1, \"O\": 2, START_TAG: 3, STOP_TAG: 4, PAD_TAG:-1}\r\n","\r\n","class my_model(nn.Module):\r\n","    def __init__(self,backbone,tag_to_ix,cls_loss=True):\r\n","        super(my_model,self).__init__()\r\n","        self.tag_to_ix = tag_to_ix\r\n","        self.tagset_size = len(tag_to_ix)\r\n","        # feature extraction\r\n","        self.backbone=backbone\r\n","        self.hidden_dim=backbone(**tokenizer('test',return_tensors='pt'))[0].shape[-1]\r\n","        # Maps the output of the backbone into tag space.\r\n","        self.hidden2tag = nn.Linear(self.hidden_dim, self.tagset_size)\r\n","        self.aux_fc = nn.Linear(self.hidden_dim,1) if cls_loss else None\r\n","        # CRF\r\n","        self.crf = CRF(self.tagset_size, batch_first=True)\r\n","        # loss func\r\n","        self.loss_fn = nn.BCEWithLogitsLoss()\r\n","        self.cls_loss=cls_loss\r\n","\r\n","    def forward(self, inputs, labels=None, cls_labels=None):\r\n","        # Get the emission scores from the backbone\r\n","        outputs = self.backbone(**inputs).last_hidden_state\r\n","        emission = self.hidden2tag(outputs)\r\n","        \r\n","        # Return result\r\n","        if labels is not None:\r\n","            crf_loss = -self.crf(nn.functional.log_softmax(emission,2), labels, mask=inputs['attention_mask'].bool(), reduction='mean')\r\n","            if self.cls_loss:\r\n","                cls_output = self.aux_fc(outputs[:,0,:])\r\n","                cls_loss = self.loss_fn(cls_output,cls_labels)\r\n","                loss = crf_loss+cls_loss\r\n","                return loss\r\n","            else:\r\n","                return crf_loss\r\n","        else:\r\n","            prediction = self.crf.decode(emission,mask=inputs['attention_mask'].bool())\r\n","            return prediction\r\n","\r\n","def gen_label(text,label):\r\n","    encoded_text=[tokenizer.cls_token] + tokenizer.tokenize(text) + [tokenizer.sep_token]\r\n","    result=[tokenizer.cls_token] + ['O']*len(tokenizer.tokenize(text)) + [tokenizer.sep_token]\r\n","    for label in label:\r\n","        if label=='':\r\n","            continue\r\n","        encoded_label=tokenizer.tokenize(label)\r\n","        for i,token in enumerate(encoded_text):\r\n","            if token==encoded_label[0] and encoded_text[i:i+len(encoded_label)]==encoded_label:\r\n","                result[i]='B'\r\n","                result[i+1:i+len(encoded_label)]=['I']*(len(encoded_label)-1)\r\n","    return [tag_to_ix[i] for i in result]\r\n","\r\n","def gen_label_batch(texts,labels):\r\n","    tags=[gen_label(*inputs)[:max_len] for inputs in zip(texts,labels)]\r\n","    max_length=max([len(tag) for tag in tags])\r\n","    if tokenizer.padding_side=='right':\r\n","        return torch.tensor([tag+[tag_to_ix[PAD_TAG]]*(max_length-len(tag)) for tag in tags], dtype=torch.long, device=device).view(len(texts),-1)\r\n","    else:\r\n","        return torch.tensor([[tag_to_ix[PAD_TAG]]*(max_length-len(tag))+tag for tag in tags], dtype=torch.long, device=device).view(len(texts),-1)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2021-06-03T20:59:03.32785Z","iopub.status.busy":"2021-06-03T20:59:03.3275Z","iopub.status.idle":"2021-06-03T20:59:03.333258Z","shell.execute_reply":"2021-06-03T20:59:03.332354Z","shell.execute_reply.started":"2021-06-03T20:59:03.327815Z"},"trusted":true},"outputs":[],"source":["class my_ds(torch.utils.data.Dataset):\r\n","    def __init__(self,text,label,cls_label):\r\n","        super().__init__()\r\n","        self.text=text\r\n","        self.label=label\r\n","        self.cls_label=cls_label\r\n","    \r\n","    def __len__(self):\r\n","        return len(self.text)\r\n","    \r\n","    def __getitem__(self,idx):\r\n","        return self.text.iloc[idx], self.label.iloc[idx], self.cls_label.iloc[idx]"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2021-06-03T20:59:03.334856Z","iopub.status.busy":"2021-06-03T20:59:03.334499Z","iopub.status.idle":"2021-06-03T20:59:03.34268Z","shell.execute_reply":"2021-06-03T20:59:03.341994Z","shell.execute_reply.started":"2021-06-03T20:59:03.334823Z"},"trusted":true},"outputs":[],"source":["train_ds=my_ds(X_train,y_train,y_train_cls)\r\n","val_ds=my_ds(X_val,y_val,y_val_cls)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2021-06-03T20:59:03.345675Z","iopub.status.busy":"2021-06-03T20:59:03.345296Z","iopub.status.idle":"2021-06-03T20:59:03.351918Z","shell.execute_reply":"2021-06-03T20:59:03.351209Z","shell.execute_reply.started":"2021-06-03T20:59:03.345637Z"},"trusted":true},"outputs":[],"source":["batch_size=4\r\n","max_len=128\r\n","\r\n","train_loader=torch.utils.data.DataLoader(train_ds,shuffle=True,batch_size=batch_size)\r\n","val_loader=torch.utils.data.DataLoader(val_ds,shuffle=False,batch_size=batch_size)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2021-06-03T20:59:03.353597Z","iopub.status.busy":"2021-06-03T20:59:03.353222Z","iopub.status.idle":"2021-06-03T20:59:08.197207Z","shell.execute_reply":"2021-06-03T20:59:08.196363Z","shell.execute_reply.started":"2021-06-03T20:59:03.353568Z"},"trusted":true},"outputs":[],"source":["from transformers import get_scheduler, AdamW\r\n","\r\n","def train_fn(train_loader,model,optimizer,lr_scheduler):\r\n","    model.train()\r\n","    train_loss=0\r\n","    train_epoch=tqdm(train_loader, total = len(train_loader), leave=False)\r\n","\r\n","    for index, batch in enumerate(train_epoch):\r\n","        # gen input\r\n","        text=list(batch[0])\r\n","        inputs=tokenizer(text,return_tensors='pt',padding=True,truncation=True,max_length=max_len)\r\n","        inputs={k:v.to(device) for k,v in inputs.items()}\r\n","        # gen label\r\n","        data_names=[label.strip().split('|') for label in batch[1]]\r\n","        tags=gen_label_batch(text,data_names)\r\n","        # cls label\r\n","        cls_labels=batch[2].view(-1,1).float().to(device)\r\n","        \r\n","        # get loss\r\n","        loss = model(inputs, tags, cls_labels)\r\n","        # optimizing\r\n","        loss.backward()\r\n","        optimizer.step()   \r\n","        lr_scheduler.step()\r\n","        optimizer.zero_grad()\r\n","             \r\n","        train_loss += loss.detach()\r\n","\r\n","        if index % 10 == 0:\r\n","            train_epoch.set_description('Step:{} | Loss:{:.3f}'.format(index,loss.item()))\r\n","    return train_loss.mean()\r\n","\r\n","def eval_fn(val_loader,model):\r\n","    model.eval()\r\n","    eval_loss=0\r\n","    \r\n","    with torch.no_grad():\r\n","        for index, batch in enumerate(tqdm(val_loader, total = len(val_loader), leave=False)):\r\n","            # gen input\r\n","            text=list(batch[0])\r\n","            inputs=tokenizer(text,return_tensors='pt',padding=True,truncation=True,max_length=max_len)\r\n","            inputs={k:v.to(device) for k,v in inputs.items()}\r\n","            # gen label\r\n","            data_names=[label.strip().split('|') for label in batch[1]]\r\n","            tags=gen_label_batch(text,data_names)\r\n","            # cls label\r\n","            cls_labels=batch[2].view(-1,1).float().to(device)\r\n","            \r\n","            # get loss\r\n","            loss = model(inputs, tags, cls_labels)\r\n","            \r\n","            eval_loss += loss.detach()\r\n","            \r\n","    return eval_loss.mean()\r\n","\r\n","def train_engine(model, epoch, train_loader, val_loader):\r\n","    # model = torch.nn.DataParallel(model)\r\n","    optimizer = AdamW(model.parameters(), lr=1e-4)\r\n","    \r\n","    num_training_steps = epoch * len(train_loader)\r\n","    lr_scheduler = get_scheduler(\r\n","        \"linear\",\r\n","        optimizer=optimizer,\r\n","        num_warmup_steps=100,\r\n","        num_training_steps=num_training_steps\r\n","    )\r\n","    \r\n","    best_eval_loss = np.inf\r\n","    for i in tqdm(range(epoch)):\r\n","        train_loss = train_fn(train_loader,model,optimizer,lr_scheduler)\r\n","        eval_loss = eval_fn(val_loader, model)\r\n","        \r\n","        print(f\"Epoch {i} , Train loss: {train_loss}, Eval loss: {eval_loss}\")\r\n","\r\n","        if eval_loss < best_eval_loss:\r\n","            best_eval_loss = eval_loss           \r\n","            \r\n","            print(\"Saving the model\")\r\n","            torch.save(model.state_dict(), f'model_checkpoint/{model_checkpoint}.bin')\r\n","            \r\n","#     return model, eval_predictions, true_labels \r\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["backbone=AutoModel.from_pretrained(model_checkpoint)\r\n","model=my_model(backbone,tag_to_ix).to(device)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2021-06-03T20:59:08.198782Z","iopub.status.busy":"2021-06-03T20:59:08.198434Z","iopub.status.idle":"2021-06-03T22:17:47.55043Z","shell.execute_reply":"2021-06-03T22:17:47.549428Z","shell.execute_reply.started":"2021-06-03T20:59:08.198729Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ab670a53a8c54eeb917f70b206c9a9da","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/10 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0872a10570594c4eb940dc11511c008a","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/17146 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (942 > 512). Running this sequence through the model will result in indexing errors\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f5ffb3e3f18d4235b8cc8bccda5cffcb","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/5716 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 0 , Train loss: 90757.8359375, Eval loss: 14170.77734375\n","Saving the model\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7ef2489137704471a615e4e733911a34","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/17146 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fa3a64ac7b764ade94579abd981af329","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/5716 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 1 , Train loss: 25551.23828125, Eval loss: 7176.58251953125\n","Saving the model\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"554e68f173734573aa5c5c18d4e62d55","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/17146 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ca50cafe02f34dd69ce22c26cb09ef5d","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/5716 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 2 , Train loss: 16535.87890625, Eval loss: 5907.7978515625\n","Saving the model\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e04666e8724f4212aabd81017acec2ae","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/17146 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5a6101633e7545c3921ec9b76139d6f0","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/5716 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 3 , Train loss: 14898.8056640625, Eval loss: 6139.64892578125\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7ea601cccae844b3a40d30309aa22aa5","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/17146 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7d26731b76db488296cb1966b6d9a914","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/5716 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 4 , Train loss: 14412.4169921875, Eval loss: 6054.23095703125\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ad95acc3dd7d4ca69ec646d7f024c11d","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/17146 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d06caa36213f4de8a50c15ca5778919c","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/5716 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 5 , Train loss: 14121.0439453125, Eval loss: 6393.51904296875\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6b5394821ca54cd0a36f5b8a683d66d6","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/17146 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["train_engine(model,10,train_loader,val_loader)"]}],"metadata":{"interpreter":{"hash":"c3192402d234f8e08a3e8523ed47572241df405ce41d7c524e9d780eaecba9b5"},"kernelspec":{"display_name":"Python 3.7.7 64-bit","name":"python3"},"language_info":{"name":"python","version":""},"orig_nbformat":3},"nbformat":4,"nbformat_minor":4}