{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2021-06-03T20:58:21.479328Z","iopub.status.busy":"2021-06-03T20:58:21.47903Z","iopub.status.idle":"2021-06-03T20:58:41.287842Z","shell.execute_reply":"2021-06-03T20:58:41.286981Z","shell.execute_reply.started":"2021-06-03T20:58:21.479257Z"},"scrolled":true,"trusted":true},"outputs":[{"data":{"text/plain":["<torch._C.Generator at 0x24680ed05a0>"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import warnings\r\n","warnings.filterwarnings('ignore', category=DeprecationWarning)\r\n","warnings.filterwarnings('ignore', category=FutureWarning)\r\n","\r\n","import os\r\n","import re\r\n","import json\r\n","import glob\r\n","from copy import deepcopy\r\n","from collections import defaultdict\r\n","from functools import partial\r\n","from imblearn.under_sampling import RandomUnderSampler\r\n","\r\n","import pandas as pd\r\n","import numpy as np\r\n","\r\n","from nltk import sent_tokenize\r\n","\r\n","import matplotlib.pyplot as plt\r\n","import seaborn as sns\r\n","\r\n","import unidecode\r\n","\r\n","from tqdm.notebook import tqdm\r\n","import string\r\n","\r\n","from transformers import AutoTokenizer, AutoModel\r\n","import torch\r\n","from torch import nn\r\n","from torchcrf import CRF\r\n","\r\n","from sklearn.model_selection import train_test_split\r\n","\r\n","%matplotlib inline\r\n","\r\n","device='cuda' if torch.cuda.is_available() else 'cpu'\r\n","# model_checkpoint='/kaggle/input/huggingface-bert/bert-base-uncased/'\r\n","model_checkpoint='bert-large-uncased'\r\n","tokenizer=AutoTokenizer.from_pretrained(model_checkpoint)\r\n","\r\n","def clean_text(txt):\r\n","    return [re.sub('[^A-Za-z0-9]+', ' ', t.lower()) for t in txt]\r\n","\r\n","torch.manual_seed(1)"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# train_df=pd.read_csv('../input/show-me-the-data-preparer/processed_train_df.csv')\n","# sample_sub = pd.read_csv('../input/show-me-the-data-preparer/processed_test_df.csv')\n","\n","train_df=pd.read_csv('processed_train_df.csv').reset_index()\n","sample_sub = pd.read_csv('processed_test_df.csv')\n","\n","train_df.drop_duplicates(inplace=True)\n","train_df.dropna(inplace=True)\n","# train_df.label=train_df.label.fillna('')\n","# labels=[int(len(label)>0) for label in train_df.label]\n","\n","# rus=RandomUnderSampler()\n","# X_res, y_res = rus.fit_resample(train_df.iloc[:,:2], labels)\n","# train_df=pd.merge(train_df,X_res.drop('text',axis=1),on='index').set_index('index')\n","\n","train_df=train_df[[len(str(t)) > 50 for t in train_df.text]]\n","\n","X_train, X_val, y_train, y_val = train_test_split(train_df.text, train_df.label, random_state=1821)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["START_TAG=tokenizer.cls_token\r\n","STOP_TAG=tokenizer.sep_token\r\n","PAD_TAG=tokenizer.pad_token\r\n","\r\n","tag_to_ix = {\"B\": 0, \"I\": 1, \"O\": 2, START_TAG: 3, STOP_TAG: 4, PAD_TAG:-1}\r\n","\r\n","def argmax(vec):\r\n","    # return the argmax as a python int\r\n","    _, idx = torch.max(vec, 1)\r\n","    return idx.item()\r\n","\r\n","\r\n","# Compute log sum exp in a numerically stable way for the forward algorithm\r\n","def log_sum_exp(vec):\r\n","    max_score = vec[0, argmax(vec)]\r\n","    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\r\n","    return max_score + \\\r\n","        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\r\n","\r\n","class my_model(nn.Module):\r\n","    def __init__(self,backbone,tag_to_ix):\r\n","        super(my_model,self).__init__()\r\n","        self.tag_to_ix = tag_to_ix\r\n","        self.tagset_size = len(tag_to_ix)\r\n","        # feature extraction\r\n","        self.backbone=backbone\r\n","        self.hidden_dim=backbone(**tokenizer('test',return_tensors='pt'))[0].shape[-1]\r\n","        # Maps the output of the backbone into tag space.\r\n","        self.hidden2tag = nn.Linear(self.hidden_dim, self.tagset_size)\r\n","        self.aux_fc = nn.Linear(self.hidden_dim,1)\r\n","        # CRF\r\n","        self.crf = CRF(self.tagset_size, batch_first=True)\r\n","\r\n","    def forward(self, inputs, labels=None):\r\n","        # Get the emission scores from the backbone\r\n","        outputs = self.backbone(**inputs)[0]\r\n","        emission = self.hidden2tag(outputs)\r\n","        # Return result\r\n","        if labels is not None:\r\n","            loss = -self.crf(nn.functional.log_softmax(emission,2), labels, mask=inputs['attention_mask'].bool(), reduction='mean')\r\n","            return loss\r\n","        else:\r\n","            prediction = self.crf.decode(emission,mask=inputs['attention_mask'].bool())\r\n","            return prediction\r\n","\r\n","def gen_label(text,label):\r\n","    encoded_text=[tokenizer.cls_token] + tokenizer.tokenize(text) + [tokenizer.sep_token]\r\n","    result=[tokenizer.cls_token] + ['O']*len(tokenizer.tokenize(text)) + [tokenizer.sep_token]\r\n","    for label in label:\r\n","        if label=='':\r\n","            continue\r\n","        encoded_label=tokenizer.tokenize(label)\r\n","        for i,token in enumerate(encoded_text):\r\n","            if token==encoded_label[0] and encoded_text[i:i+len(encoded_label)]==encoded_label:\r\n","                result[i]='B'\r\n","                result[i+1:i+len(encoded_label)]=['I']*(len(encoded_label)-1)\r\n","    return [tag_to_ix[i] for i in result]\r\n","\r\n","def gen_label_batch(texts,labels):\r\n","    tags=[gen_label(*inputs)[:max_len] for inputs in zip(texts,labels)]\r\n","    max_length=max([len(tag) for tag in tags])\r\n","    if tokenizer.padding_side=='right':\r\n","        return torch.tensor([tag+[tag_to_ix[PAD_TAG]]*(max_length-len(tag)) for tag in tags], dtype=torch.long, device=device).view(len(texts),-1)\r\n","    else:\r\n","        return torch.tensor([[tag_to_ix[PAD_TAG]]*(max_length-len(tag))+tag for tag in tags], dtype=torch.long, device=device).view(len(texts),-1)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2021-06-03T20:59:03.32785Z","iopub.status.busy":"2021-06-03T20:59:03.3275Z","iopub.status.idle":"2021-06-03T20:59:03.333258Z","shell.execute_reply":"2021-06-03T20:59:03.332354Z","shell.execute_reply.started":"2021-06-03T20:59:03.327815Z"},"trusted":true},"outputs":[],"source":["class my_ds(torch.utils.data.Dataset):\n","    def __init__(self,text,label):\n","        super().__init__()\n","        self.text=text\n","        self.label=label\n","    \n","    def __len__(self):\n","        return len(self.text)\n","    \n","    def __getitem__(self,idx):\n","        return self.text.iloc[idx], self.label.iloc[idx]"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2021-06-03T20:59:03.334856Z","iopub.status.busy":"2021-06-03T20:59:03.334499Z","iopub.status.idle":"2021-06-03T20:59:03.34268Z","shell.execute_reply":"2021-06-03T20:59:03.341994Z","shell.execute_reply.started":"2021-06-03T20:59:03.334823Z"},"trusted":true},"outputs":[],"source":["train_ds=my_ds(X_train,y_train)\n","val_ds=my_ds(X_val,y_val)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2021-06-03T20:59:03.345675Z","iopub.status.busy":"2021-06-03T20:59:03.345296Z","iopub.status.idle":"2021-06-03T20:59:03.351918Z","shell.execute_reply":"2021-06-03T20:59:03.351209Z","shell.execute_reply.started":"2021-06-03T20:59:03.345637Z"},"trusted":true},"outputs":[],"source":["train_loader=torch.utils.data.DataLoader(train_ds,shuffle=True,batch_size=4)\r\n","val_loader=torch.utils.data.DataLoader(val_ds,shuffle=False,batch_size=4)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2021-06-03T20:59:03.353597Z","iopub.status.busy":"2021-06-03T20:59:03.353222Z","iopub.status.idle":"2021-06-03T20:59:08.197207Z","shell.execute_reply":"2021-06-03T20:59:08.196363Z","shell.execute_reply.started":"2021-06-03T20:59:03.353568Z"},"trusted":true},"outputs":[],"source":["from transformers import get_scheduler, AdamW\r\n","\r\n","max_len=128\r\n","\r\n","def train_fn(train_loader,model,optimizer,lr_scheduler):\r\n","    model.train()\r\n","    train_loss=0\r\n","    train_epoch=tqdm(train_loader, total = len(train_loader), leave=False)\r\n","\r\n","    for index, batch in enumerate(train_epoch):\r\n","        # gen input\r\n","        text=list(batch[0])\r\n","        inputs=tokenizer(text,return_tensors='pt',padding=True,truncation=True,max_length=max_len)\r\n","        inputs={k:v.to(device) for k,v in inputs.items()}\r\n","        # gen label\r\n","        data_names=[label.strip().split('|') for label in batch[1]]\r\n","        labels=torch.tensor([len(label)>0 for label in data_names],dtype=torch.float,device=device).view(len(data_names),-1)\r\n","        tags=gen_label_batch(text,data_names)\r\n","        \r\n","        # get loss\r\n","        loss = model(inputs, tags)\r\n","        \r\n","        # optimizing\r\n","        loss.backward()\r\n","        optimizer.step()   \r\n","        lr_scheduler.step()\r\n","        optimizer.zero_grad()\r\n","             \r\n","        train_loss += loss.detach()\r\n","\r\n","        if index % 10 == 0:\r\n","            train_epoch.set_description('Step:{} | Loss:{:.3f}'.format(index,loss.item()))\r\n","    return train_loss.sum()\r\n","\r\n","def eval_fn(val_loader,model):\r\n","    model.eval()\r\n","    eval_loss=0\r\n","    \r\n","    with torch.no_grad():\r\n","        for index, batch in enumerate(tqdm(val_loader, total = len(val_loader), leave=False)):\r\n","            # gen input\r\n","            text=list(batch[0])\r\n","            inputs=tokenizer(text,return_tensors='pt',padding=True,truncation=True,max_length=max_len)\r\n","            inputs={k:v.to(device) for k,v in inputs.items()}\r\n","            # gen label\r\n","            data_names=[label.strip().split('|') for label in batch[1]]\r\n","            labels=torch.tensor([len(label)>0 for label in data_names],dtype=torch.float,device=device).view(len(data_names),-1)\r\n","            tags=gen_label_batch(text,data_names)\r\n","            \r\n","            # get loss\r\n","            loss = model(inputs, tags)\r\n","            \r\n","            eval_loss += loss.detach()\r\n","            \r\n","    return eval_loss.sum()\r\n","\r\n","def train_engine(model, epoch, train_loader, val_loader):\r\n","    # model = torch.nn.DataParallel(model)\r\n","    optimizer = AdamW(model.parameters(), lr=1e-4)\r\n","    \r\n","    num_training_steps = epoch * len(train_loader)\r\n","    lr_scheduler = get_scheduler(\r\n","        \"linear\",\r\n","        optimizer=optimizer,\r\n","        num_warmup_steps=100,\r\n","        num_training_steps=num_training_steps\r\n","    )\r\n","    \r\n","    best_eval_loss = np.inf\r\n","    for i in tqdm(range(epoch)):\r\n","        train_loss = train_fn(train_loader,model,optimizer,lr_scheduler)\r\n","        eval_loss = eval_fn(val_loader, model)\r\n","        \r\n","        print(f\"Epoch {i} , Train loss: {train_loss}, Eval loss: {eval_loss}\")\r\n","\r\n","        if eval_loss < best_eval_loss:\r\n","            best_eval_loss = eval_loss           \r\n","            \r\n","            print(\"Saving the model\")\r\n","            torch.save(model.state_dict(), model_checkpoint+'.bin')\r\n","            \r\n","#     return model, eval_predictions, true_labels \r\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["backbone=AutoModel.from_pretrained(model_checkpoint)\r\n","model=my_model(backbone,tag_to_ix).to(device)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2021-06-03T20:59:08.198782Z","iopub.status.busy":"2021-06-03T20:59:08.198434Z","iopub.status.idle":"2021-06-03T22:17:47.55043Z","shell.execute_reply":"2021-06-03T22:17:47.549428Z","shell.execute_reply.started":"2021-06-03T20:59:08.198729Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c7564f17e7d94d509ed4e49ed27d29bc","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/10 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9b7950ff6ea94e4ab6cc5e044eb4264e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/9066 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (521 > 512). Running this sequence through the model will result in indexing errors\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"434ee9bcb21d433e92bfca2e4252a264","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3023 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 0 , Train loss: 185582.546875, Eval loss: 45582.32421875\n","Saving the model\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6b09dfea927e4aa59468506cac6c0340","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/9066 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4df66e20e7b14ca280aa9fe8e57f16bc","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3023 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 1 , Train loss: 109340.7734375, Eval loss: 49455.19140625\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9d193396045d4ef49e3e0c123f18e9e1","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/9066 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a7dc650a882d4f43ada05f009b20de62","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3023 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 2 , Train loss: 80129.6875, Eval loss: 42572.10546875\n","Saving the model\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c0655ceb154b43cda4efa6edb0ac0e29","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/9066 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a7cfed84f5984ed5b6e1c7257e9f0a5e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3023 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 3 , Train loss: 70671.2578125, Eval loss: 40256.8671875\n","Saving the model\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"eff5d99cfbf241619a1251ba6d7398c9","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/9066 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0d0377221bff4a92b43fcd706385e356","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3023 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 4 , Train loss: 67047.3984375, Eval loss: 49684.90234375\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9adb7e14aa494ec8a532011059154d1f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/9066 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"088f9164f95148d789494e8d97295af2","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3023 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 5 , Train loss: 65441.85546875, Eval loss: 57175.38671875\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"728fa8395ce24587b8eb93bd8365acc3","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/9066 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7b4548a3062f4796b57a637136c8529d","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3023 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 6 , Train loss: 64657.140625, Eval loss: 54562.5546875\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b6ee289586e34632be8d8be62aeb991d","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/9066 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e8a5af341afe45f089a7cdb908f42f60","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3023 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 7 , Train loss: 64200.07421875, Eval loss: 40169.49609375\n","Saving the model\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5cba4663f04e4ef49e5cecdd3cba54f0","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/9066 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c36ab56aae664f27ac461fae80791c25","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3023 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 8 , Train loss: 63941.83203125, Eval loss: 47015.40625\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"69923bb6ce1c42319a4bc9e6afc36de8","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/9066 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"56f6279981f24297835f8aaaa1d8dbc0","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3023 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 9 , Train loss: 63817.1328125, Eval loss: 52570.28125\n"]}],"source":["train_engine(model,10,train_loader,val_loader)"]}],"metadata":{"interpreter":{"hash":"b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"},"kernelspec":{"display_name":"Python 3.6.7 64-bit ('base': conda)","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"},"orig_nbformat":3},"nbformat":4,"nbformat_minor":4}