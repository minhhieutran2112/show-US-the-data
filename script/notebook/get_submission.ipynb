{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1778104f330>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\r\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\r\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\r\n",
    "\r\n",
    "import os\r\n",
    "import re\r\n",
    "import json\r\n",
    "import glob\r\n",
    "from copy import deepcopy\r\n",
    "from collections import defaultdict\r\n",
    "from functools import partial\r\n",
    "from imblearn.under_sampling import RandomUnderSampler\r\n",
    "\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "from nltk import sent_tokenize\r\n",
    "\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "\r\n",
    "import unidecode\r\n",
    "\r\n",
    "from tqdm.notebook import tqdm\r\n",
    "import string\r\n",
    "\r\n",
    "from transformers import AutoTokenizer, AutoModel\r\n",
    "import torch\r\n",
    "from torch import nn\r\n",
    "from torchcrf import CRF\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "%matplotlib inline\r\n",
    "\r\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'\r\n",
    "model_checkpoint='distilbert-base-uncased'\r\n",
    "# model_checkpoint='distilbert-base-uncased'\r\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_checkpoint)\r\n",
    "\r\n",
    "def clean_text(txt):\r\n",
    "    return [re.sub('[^A-Za-z0-9]+', ' ', str(t).lower()) for t in txt]\r\n",
    "torch.manual_seed(1)\r\n",
    "max_len=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=pd.read_csv('data/processed_data/processed_train_df.csv')\r\n",
    "labels_list=[i.split('|') for i in train_df.label.dropna().unique()]\r\n",
    "labels_list=np.unique([i for label in labels_list for i in label])\r\n",
    "input_path='data/raw_data/'\r\n",
    "basepath=input_path+'coleridgeinitiative-show-us-the-data/'\r\n",
    "train_df=pd.read_csv(basepath+'train.csv')\r\n",
    "sample_sub = pd.read_csv(basepath+'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8f75a8fdc6d424c88819c326520c493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19661 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2b99a0fcb9647ddaad5a1ceef245579",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19661 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f13819fc4d7e4e25832ff1309be2f5a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19661 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_files_path=basepath+'train/'\r\n",
    "test_files_path=basepath+'test/'\r\n",
    "def read_append_return(filename, train_files_path=train_files_path, output='text'):\r\n",
    "    json_path = os.path.join(train_files_path, (filename+'.json'))\r\n",
    "    headings = []\r\n",
    "    contents = []\r\n",
    "    combined = []\r\n",
    "    with open(json_path, 'r') as f:\r\n",
    "        json_decode = json.load(f)\r\n",
    "        for data in json_decode:\r\n",
    "            headings.extend(sent_tokenize(unidecode.unidecode(data.get('section_title'))))\r\n",
    "            contents.extend(sent_tokenize(unidecode.unidecode(data.get('text'))))\r\n",
    "            combined.extend(sent_tokenize(unidecode.unidecode(data.get('section_title'))))\r\n",
    "            combined.extend(sent_tokenize(unidecode.unidecode(data.get('text'))))\r\n",
    "    \r\n",
    "    if output == 'text':\r\n",
    "        return contents\r\n",
    "    elif output == 'head':\r\n",
    "        return headings\r\n",
    "    else:\r\n",
    "        return combined\r\n",
    "    \r\n",
    "tqdm.pandas()\r\n",
    "sample_sub['text'] = sample_sub['Id'].progress_apply(partial(read_append_return, train_files_path=test_files_path))\r\n",
    "\r\n",
    "sample_sub['cleaned_text']=sample_sub.text.progress_apply(clean_text)\r\n",
    "\r\n",
    "test_texts=[[t.strip() for t in txt] for txt in sample_sub.cleaned_text]\r\n",
    "\r\n",
    "sub_labels=[]\r\n",
    "for text in tqdm(sample_sub.cleaned_text):\r\n",
    "    text=' '.join(text)\r\n",
    "    tmp=sorted([label.strip() for label in labels_list if label in text],key=lambda x: len(x))\r\n",
    "    sub_labels.append(tmp)\r\n",
    "\r\n",
    "test_df=pd.DataFrame(zip(test_texts,sample_sub.Id),columns=['text','Id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TAG=tokenizer.cls_token\r\n",
    "STOP_TAG=tokenizer.sep_token\r\n",
    "PAD_TAG=tokenizer.pad_token\r\n",
    "\r\n",
    "tag_to_ix = {\"B\": 0, \"I\": 1, \"O\": 2, START_TAG: 3, STOP_TAG: 4, PAD_TAG:-1}\r\n",
    "\r\n",
    "class my_model(nn.Module):\r\n",
    "    def __init__(self,backbone,tag_to_ix):\r\n",
    "        super(my_model,self).__init__()\r\n",
    "        self.tag_to_ix = tag_to_ix\r\n",
    "        self.tagset_size = len(tag_to_ix)\r\n",
    "        # feature extraction\r\n",
    "        self.backbone=backbone\r\n",
    "        self.hidden_dim=backbone(**tokenizer('test',return_tensors='pt'))[0].shape[-1]\r\n",
    "        # Maps the output of the backbone into tag space.\r\n",
    "        self.hidden2tag = nn.Linear(self.hidden_dim, self.tagset_size)\r\n",
    "        self.aux_fc = nn.Linear(self.hidden_dim,1)\r\n",
    "        # CRF\r\n",
    "        self.crf = CRF(self.tagset_size, batch_first=True)\r\n",
    "        # loss func\r\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\r\n",
    "\r\n",
    "    def forward(self, inputs, labels=None, cls_labels=None):\r\n",
    "        # Get the emission scores from the backbone\r\n",
    "        outputs = self.backbone(**inputs).last_hidden_state\r\n",
    "        emission = self.hidden2tag(outputs)\r\n",
    "        cls_output = self.aux_fc(outputs[:,0,:])\r\n",
    "        \r\n",
    "        # Return result\r\n",
    "        if labels is not None and cls_labels is not None:\r\n",
    "            crf_loss = -self.crf(nn.functional.log_softmax(emission,2), labels, mask=inputs['attention_mask'].bool(), reduction='mean')\r\n",
    "            cls_loss = self.loss_fn(cls_output,cls_labels)\r\n",
    "            loss = crf_loss+cls_loss\r\n",
    "            return loss\r\n",
    "        else:\r\n",
    "            prediction = self.crf.decode(emission,mask=inputs['attention_mask'].bool())\r\n",
    "            return prediction\r\n",
    "\r\n",
    "def gen_label(text,label):\r\n",
    "    encoded_text=[tokenizer.cls_token] + tokenizer.tokenize(text) + [tokenizer.sep_token]\r\n",
    "    result=[tokenizer.cls_token] + ['O']*len(tokenizer.tokenize(text)) + [tokenizer.sep_token]\r\n",
    "    for label in label:\r\n",
    "        if label=='':\r\n",
    "            continue\r\n",
    "        encoded_label=tokenizer.tokenize(label)\r\n",
    "        for i,token in enumerate(encoded_text):\r\n",
    "            if token==encoded_label[0] and encoded_text[i:i+len(encoded_label)]==encoded_label:\r\n",
    "                result[i]='B'\r\n",
    "                result[i+1:i+len(encoded_label)]=['I']*(len(encoded_label)-1)\r\n",
    "    return [tag_to_ix[i] for i in result]\r\n",
    "\r\n",
    "def gen_label_batch(texts,labels):\r\n",
    "    tags=[gen_label(*inputs)[:max_len] for inputs in zip(texts,labels)]\r\n",
    "    max_length=max([len(tag) for tag in tags])\r\n",
    "    if tokenizer.padding_side=='right':\r\n",
    "        return torch.tensor([tag+[tag_to_ix[PAD_TAG]]*(max_length-len(tag)) for tag in tags], dtype=torch.long, device=device).view(len(texts),-1)\r\n",
    "    else:\r\n",
    "        return torch.tensor([[tag_to_ix[PAD_TAG]]*(max_length-len(tag))+tag for tag in tags], dtype=torch.long, device=device).view(len(texts),-1)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backbone=AutoModel.from_pretrained(model_checkpoint)\r\n",
    "model=my_model(backbone,tag_to_ix).to(device)\r\n",
    "model.load_state_dict(torch.load('model_checkpoint/bert-large-uncased.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\r\n",
    "def get_result(inputs,outputs):\r\n",
    "    try:\r\n",
    "        outputs_txt=''.join([str(i) for i in outputs])\r\n",
    "        groups=[m.span() for m in re.finditer(r'[01]+',outputs_txt)]\r\n",
    "        prediction=[]\r\n",
    "        for i,j in groups:\r\n",
    "            while tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][i].item()).startswith('##'):\r\n",
    "                i-=1\r\n",
    "            while tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][j].item()).startswith('##'):\r\n",
    "                j+=1\r\n",
    "            prediction.append(tokenizer.decode(inputs['input_ids'][0][i:j]))\r\n",
    "        return prediction\r\n",
    "    except:\r\n",
    "        return []\r\n",
    "\r\n",
    "def filter_labels(labels):\r\n",
    "    tmp=[]\r\n",
    "    labels=np.unique(labels)\r\n",
    "    for index,label in enumerate(labels):\r\n",
    "        try:\r\n",
    "            if sum([label in ref for ref in labels])>1:\r\n",
    "                continue\r\n",
    "            else:\r\n",
    "                tmp.append(str(label).strip())\r\n",
    "        except:\r\n",
    "            tmp.append(str(label).strip())\r\n",
    "    return '|'.join(sorted(tmp))\r\n",
    "\r\n",
    "sigmoid=nn.Sigmoid()\r\n",
    "\r\n",
    "def get_cls_label(inputs):\r\n",
    "    outputs = model.backbone(**inputs).last_hidden_state\r\n",
    "    cls_output = model.aux_fc(outputs[:,0,:])\r\n",
    "    return sigmoid(cls_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33a0f57063854af0bedfcfbde9632fbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19661 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get prediction\r\n",
    "predictions=[]\r\n",
    "for i,txt in enumerate(tqdm(test_df.text)):\r\n",
    "    tmp_pred=[]\r\n",
    "    for text in txt:\r\n",
    "        inputs=tokenizer(text,return_tensors='pt',padding=True,truncation=True,max_length=max_len)\r\n",
    "        inputs={k:v.to(device) for k,v in inputs.items()}\r\n",
    "        if get_cls_label(inputs)[0].item() < 0.5:\r\n",
    "            continue\r\n",
    "        outputs=np.array(model(inputs)[0])\r\n",
    "        tmp_pred.extend(get_result(inputs,outputs))\r\n",
    "    predictions.append(filter_labels(sorted(clean_text(tmp_pred+sub_labels[i]))))\r\n",
    "\r\n",
    "test_df['PredictionString']=predictions\r\n",
    "submission=test_df.drop('text',axis=1)\r\n",
    "submission.to_csv('data/processed_data/submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.6.7 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}